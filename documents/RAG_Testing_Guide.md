# RAG Testing Guide: Systematic Exploration Scripts

## Overview
This guide provides a structured approach to testing and understanding your RAG system using the comprehensive interface we've built. The guide is divided into **Core Tests** for demonstrations and **Extra Testing** for extended exploration and experimentation.

---

## üéØ **Testing Philosophy**

Each test is designed to:
- **Validate** a specific aspect of the RAG system
- **Teach** you something about how RAG works
- **Reveal** optimization opportunities
- **Build** your intuition about parameter effects
- **Demonstrate** advanced features like query expansion

---

## üìã **Pre-Test Setup**

Before starting any test series:
1. **Clear all data** using the "üóëÔ∏è Clear All Data" button
2. **Set baseline parameters** using "üéØ Precise Mode" preset
3. **Have test documents ready** - We have 4 excellent documents available:
   - `empire_state_knowledge.md` - 1930s skyscraper
   - `golden_gate_bridge.md` - 1930s bridge engineering
   - `hoover_dam.md` - 1930s concrete dam
   - `international_space_station.md` - Modern space engineering
4. **Enable Query Expansion** (default ON) in Parameters tab

---

# üèÜ **CORE TESTS** (For Demonstrations)

## üß™ **TEST SERIES 1: BASIC RAG VALIDATION**

### Test 1.1: Baseline vs RAG Comparison
**Objective:** Prove that RAG actually improves answers with relevant context

**What You'll Learn:** Whether RAG provides measurable improvement over general knowledge

**Steps:**
1. **Ask without documents:**
   - Go to "üí¨ Ask Questions" tab
   - Ask: "How tall is the Golden Gate Bridge?"
   - Note the answer type (should be "General")
   - Save the response

2. **Upload Golden Gate Bridge document:**
   - Go to "üìÅ Documents" tab
   - Upload `golden_gate_bridge.md` from demo_context_files
   - Process documents with default settings

3. **Ask the same question:**
   - Return to "üí¨ Ask Questions" tab
   - Ask the exact same question
   - Note the answer type (should be "RAG")
   - Compare specificity and accuracy

**Expected Results:**
- General answer: Vague or potentially incorrect
- RAG answer: Specific "746 feet (227 meters) above water"

**What This Proves:** RAG retrieval provides more accurate, contextual answers

---

### Test 1.2: Query Expansion Effectiveness
**Objective:** Demonstrate how query expansion improves natural language question handling

**What You'll Learn:** Why different question phrasings can yield different results

**Steps:**
1. **Test with query expansion ON (default):**
   - Ensure "üîç Enable Query Expansion" is checked in Parameters
   - Ask: "How tall is the tallest tower of the Golden Gate Bridge?"
   - Go to "üîç RAG Details" to see retrieved chunks
   - Note the answer quality

2. **Test with query expansion OFF:**
   - Uncheck "üîç Enable Query Expansion" in Parameters
   - Ask the same question
   - Compare retrieval results and answer quality

3. **Test keyword-style query:**
   - Ask: "Golden Gate Bridge tower height"
   - Compare with the natural language version

**Expected Results:**
- Query expansion ON: Successfully finds tower height information
- Query expansion OFF: May struggle with natural language phrasing
- Keyword query: Works well regardless of expansion setting

**What This Proves:** Query expansion bridges the gap between natural language and document structure

---

## üî¨ **TEST SERIES 2: PARAMETER OPTIMIZATION**

### Test 2.1: The Golden Gate Bridge Solution
**Objective:** Demonstrate the parameter optimization process using a real-world example

**What You'll Learn:** How systematic parameter adjustment solves retrieval problems

**Steps:**
1. **Start with default parameters:**
   - Chunk Size: 500, Overlap: 100, Results: 3
   - Ask: "How tall is the tallest tower of the Golden Gate Bridge?"
   - Note if answer is incomplete or says "insufficient information"

2. **Apply the optimized solution:**
   - Change to: Chunk Size: 700, Overlap: 125, Results: 8
   - Reprocess documents
   - Ask the same question
   - Note the dramatic improvement

3. **Test the individual parameter effects:**
   - Try just increasing chunk size to 700 (keep others default)
   - Try just increasing results to 8 (keep others default)
   - See which parameter had the biggest impact

**Expected Results:**
- Default parameters: May fail to find the information
- Optimized parameters: Clear, accurate answer with proper context
- Individual testing: Reveals which parameters matter most

**What This Proves:** Systematic parameter optimization can solve challenging retrieval problems

---

## üé® **TEST SERIES 3: MULTI-DOCUMENT TESTING**

### Test 3.1: Cross-Document Comparison
**Objective:** Test the system's ability to compare information across multiple documents

**What You'll Learn:** How RAG handles multi-document queries

**Steps:**
1. **Upload multiple documents:**
   - Upload both `empire_state_knowledge.md` and `golden_gate_bridge.md`
   - Process with optimized parameters (700/125/8)

2. **Test comparative queries:**
   - Ask: "Which is taller: the Empire State Building or the Golden Gate Bridge towers?"
   - Ask: "Compare the construction timelines of these two landmarks"
   - Ask: "What materials were used in both structures?"

3. **Test document-specific queries:**
   - Ask: "What is unique about the Golden Gate Bridge's color?"
   - Verify it doesn't confuse information between documents

**Expected Results:**
- Comparative queries: Should find relevant information from both documents
- Specific queries: Should maintain document attribution
- No information bleeding between unrelated topics

**What This Proves:** The system can handle multiple documents while maintaining accuracy

---

# üöÄ **EXTRA TESTING** (Extended Exploration)

## üìä **CATEGORY A: ADVANCED PARAMETER STUDIES**

### A.1: Chunk Size Deep Dive
**Objective:** Understand optimal chunk sizes for different content types

**Test Scenarios:**
1. **Technical Specifications** (ISS document):
   - Test 200, 500, 800, 1200 character chunks
   - Ask: "What are the ISS solar array specifications?"
   - Find optimal size for dense technical content

2. **Historical Narrative** (Empire State Building):
   - Test same chunk sizes
   - Ask: "Describe the Empire State Building's construction story"
   - Compare narrative flow quality

3. **Mixed Content** (Hoover Dam):
   - Test with construction details and statistics
   - Ask: "How was the Hoover Dam's concrete cooling system designed?"
   - Find balance between detail and context

### A.2: Overlap Optimization Study
**Objective:** Find the sweet spot for chunk overlap

**Test Matrix:**
- Chunk Size: 600 (fixed)
- Overlap: 0, 50, 100, 150, 200, 250
- Query: "Describe the Golden Gate Bridge construction challenges"
- Measure: Answer completeness and flow

**Analysis Points:**
- At what overlap do answers become smooth?
- When does overlap become redundant?
- How does overlap affect processing time?

### A.3: Retrieval Count Impact Analysis
**Objective:** Determine optimal number of chunks for different query types

**Query Categories:**
1. **Simple Factual**: "When was the ISS first occupied?" (test 1-10 results)
2. **Complex Analytical**: "Compare engineering challenges across all four structures" (test 5-20 results)
3. **Synthesis**: "How do these structures represent different eras of engineering?" (test 10-30 results)

---

## üß† **CATEGORY B: QUERY COMPLEXITY STUDIES**

### B.1: Natural Language vs Keyword Optimization
**Objective:** Master query phrasing for best results

**Natural Language Variations:**
- "How tall is the tallest tower of the Golden Gate Bridge?"
- "What's the height of the Golden Gate Bridge's highest point?"
- "How high do the Golden Gate Bridge towers reach?"

**Keyword Variations:**
- "Golden Gate Bridge tower height"
- "bridge tower 746 feet"
- "tower height above water"

**Test Process:**
1. Test each variation with query expansion ON/OFF
2. Measure retrieval quality and answer accuracy
3. Document which phrasings work best

### B.2: Multi-Part Question Handling
**Objective:** Test complex queries requiring multiple pieces of information

**Progressive Complexity:**
1. **Two-part**: "What is the height and construction date of the Empire State Building?"
2. **Three-part**: "Compare the height, construction time, and cost of the Empire State Building and Hoover Dam"
3. **Synthesis**: "How do the construction challenges of the 1930s projects compare to modern ISS assembly?"

### B.3: Hypothetical and Inference Testing
**Objective:** Understand RAG limitations with questions requiring inference

**Test Questions:**
- "What would happen if the Golden Gate Bridge were built today?"
- "How might the ISS design influence future space stations?"
- "What if the Hoover Dam had been built with modern materials?"

**Expected Results:**
- RAG should acknowledge when information isn't in the context
- System should not hallucinate answers beyond provided information

---

## üî¨ **CATEGORY C: TECHNICAL SYSTEM ANALYSIS**

### C.1: Vector Similarity Deep Dive
**Objective:** Understand the relationship between similarity scores and content relevance

**Analysis Steps:**
1. **High Similarity Analysis** (scores < 0.3):
   - Find chunks with excellent similarity scores
   - Examine their actual content relevance
   - Look for false positives

2. **Medium Similarity Analysis** (scores 0.3-0.6):
   - Assess whether these chunks provide useful context
   - Identify borderline cases

3. **Low Similarity Analysis** (scores > 0.6):
   - Determine if these are noise or still valuable
   - Test if increasing retrieval count helps or hurts

### C.2: Query Expansion Mechanism Study
**Objective:** Understand exactly how query expansion works

**Detailed Analysis:**
1. **Enable query expansion** and ask: "How tall is the tallest tower of the Golden Gate Bridge?"
2. **Manually test the expanded queries** (with expansion OFF):
   - "Golden Gate Bridge tower height"
   - "tower height 746 feet"
   - "bridge tower dimensions"
3. **Compare retrieval results** for each variation
4. **Document which expansions are most effective**

### C.3: Cross-Document Retrieval Patterns
**Objective:** Understand how the system handles information from multiple sources

**Test Scenarios:**
1. **Upload all 4 documents** (Empire State, Golden Gate, Hoover Dam, ISS)
2. **Test queries that could match multiple documents**:
   - "tallest structure" (could match any)
   - "1930s construction" (matches 3 documents)
   - "engineering marvel" (matches all)
3. **Analyze which documents are retrieved** and why

---

## üéØ **CATEGORY D: DOMAIN-SPECIFIC TESTING**

### D.1: Era-Based Comparisons
**Objective:** Test the system's ability to handle temporal relationships

**1930s Engineering Projects:**
- Empire State Building (1930-1931)
- Golden Gate Bridge (1933-1937)  
- Hoover Dam (1931-1936)

**Test Queries:**
- "What were the common challenges of 1930s mega-projects?"
- "How did Great Depression affect these construction projects?"
- "Compare 1930s engineering techniques to modern ISS assembly"

### D.2: Material Science Comparisons
**Objective:** Test technical domain knowledge across documents

**Material Categories:**
- **Steel**: Empire State Building, Golden Gate Bridge
- **Concrete**: Hoover Dam, Empire State Building (mixed)
- **Advanced Materials**: ISS (aluminum, composites)

**Test Queries:**
- "Compare steel usage across these structures"
- "How do concrete and steel construction methods differ?"
- "What advanced materials are used in space construction?"

### D.3: Scale and Dimension Analysis
**Objective:** Test numerical reasoning and comparison capabilities

**Dimension Comparisons:**
- **Height**: Empire State (1,454 ft) vs Golden Gate towers (746 ft) vs Hoover Dam (726 ft)
- **Length**: Golden Gate Bridge (8,980 ft) vs ISS (357 ft)
- **Weight**: Various massive structures

**Test Queries:**
- "Rank these structures by height"
- "Which structure required the most material?"
- "Compare the scales of terrestrial vs space engineering"

---

## üåü **CATEGORY E: CREATIVE AND EDGE CASE TESTING**

### E.1: Ambiguous Query Resolution
**Objective:** Test how the system handles unclear or ambiguous questions

**Ambiguous Queries:**
- "How big is it?" (without specifying which structure)
- "When was it built?" (could refer to any structure)
- "What makes it special?" (very general)

**Expected Behavior:**
- System should ask for clarification or provide context-appropriate answers
- Should not make assumptions about which structure is referenced

### E.2: Negative and Limitation Testing
**Objective:** Verify the system properly acknowledges its limitations

**Limitation Test Queries:**
- "What is the best restaurant near the Golden Gate Bridge?" (irrelevant)
- "How do these structures compare to the Eiffel Tower?" (missing information)
- "What will these structures look like in 100 years?" (speculation)

**Expected Results:**
- Clear "insufficient information" responses
- No hallucination or speculation beyond provided context

### E.3: Multi-Language and Technical Term Testing
**Objective:** Test handling of technical terminology and varied expressions

**Technical Term Variations:**
- "ISS" vs "International Space Station"
- "ESB" vs "Empire State Building"
- "Hoover Dam" vs "Boulder Dam"
- Technical terms: "arch-gravity dam", "suspension bridge", "microgravity"

**Test Process:**
1. Use abbreviations and full names
2. Test technical vs layman terminology
3. Verify consistent retrieval regardless of term variation

---

## üìà **CATEGORY F: PERFORMANCE AND OPTIMIZATION**

### F.1: Processing Speed Analysis
**Objective:** Understand the trade-offs between quality and speed

**Speed Test Matrix:**
- **Small Setup**: 1 document, 300 chunks, 3 results
- **Medium Setup**: 2 documents, 500 chunks, 5 results  
- **Large Setup**: 4 documents, 700 chunks, 8 results
- **Maximum Setup**: 4 documents, 1000 chunks, 15 results

**Measurements:**
- Document processing time
- Query response time
- Answer quality vs speed trade-offs

### F.2: Model Comparison Study
**Objective:** Compare different OpenAI models across various query types

**Test Matrix:**
- **Models**: GPT-3.5 Turbo, GPT-4, GPT-4 Turbo
- **Query Types**: Factual, analytical, comparative, creative
- **Metrics**: Accuracy, detail level, processing time, cost

**Sample Queries for Each Model:**
1. **Factual**: "What is the height of the Empire State Building?"
2. **Analytical**: "Analyze the engineering innovations in the Hoover Dam"
3. **Comparative**: "Compare the construction challenges of 1930s vs modern projects"
4. **Creative**: "Describe what makes these structures iconic"

### F.3: Parameter Preset Validation
**Objective:** Validate and potentially improve the built-in parameter presets

**Preset Testing:**
1. **Precise Mode**: Test with factual queries across all documents
2. **Balanced Mode**: Test with mixed query types
3. **Creative Mode**: Test with open-ended, interpretive queries

**Optimization Opportunity:**
- Document which presets work best for which query types
- Suggest improvements to preset configurations

---

## üéì **CATEGORY G: EDUCATIONAL AND DEMONSTRATION SCENARIOS**

### G.1: Progressive Complexity Demonstrations
**Objective:** Create a learning progression from simple to complex

**Level 1 - Basic Facts:**
- "When was the Empire State Building completed?"
- "How tall is the Golden Gate Bridge?"

**Level 2 - Comparisons:**
- "Which is taller: Empire State Building or Golden Gate Bridge towers?"
- "Compare construction times of 1930s projects"

**Level 3 - Analysis:**
- "What were the major engineering challenges of each project?"
- "How do these structures represent different engineering approaches?"

**Level 4 - Synthesis:**
- "How do these projects demonstrate the evolution of engineering?"
- "What lessons from these projects apply to modern construction?"

### G.2: Error Recovery and Troubleshooting Scenarios
**Objective:** Demonstrate systematic problem-solving approaches

**Common Problems and Solutions:**
1. **"Information not found"** ‚Üí Increase chunk size and retrieval count
2. **"Irrelevant context"** ‚Üí Reduce retrieval count, improve query phrasing
3. **"Incomplete answers"** ‚Üí Increase chunk overlap, check document coverage
4. **"Inconsistent results"** ‚Üí Lower temperature, enable query expansion

### G.3: Real-World Application Scenarios
**Objective:** Show how RAG applies to practical use cases

**Scenario Examples:**
1. **Research Assistant**: "Find all mentions of construction costs across documents"
2. **Fact Checker**: "Verify the height claims for each structure"
3. **Comparative Analyst**: "Create a timeline of all construction projects"
4. **Technical Writer**: "Extract key specifications for each structure"

---

## üîß **TESTING UTILITIES AND TOOLS**

### Quick Reference Commands
- **Reset Everything**: Clear All Data ‚Üí Upload Documents ‚Üí Set Parameters
- **Parameter Presets**: Use for consistent starting points
- **Export Results**: Save test results for analysis
- **Vector Explorer**: Examine chunk-level retrieval details

### Systematic Testing Approach
1. **Hypothesis**: What do you expect to happen?
2. **Test**: Run the specific test scenario
3. **Observe**: Note actual results vs expectations
4. **Analyze**: Why did you get these results?
5. **Document**: Record findings for future reference

### Performance Benchmarks
- **Good Retrieval**: Similarity scores < 0.4, relevant content
- **Acceptable Speed**: < 5 seconds for query processing
- **Quality Threshold**: Accurate, complete answers with proper attribution

---

## üìã **TESTING CHECKLIST**

### Before Each Test Session:
- [ ] Clear all previous data
- [ ] Set known parameter baseline
- [ ] Upload appropriate test documents
- [ ] Verify query expansion setting

### During Testing:
- [ ] Document parameter settings used
- [ ] Note query phrasing exactly
- [ ] Record retrieval scores and chunks
- [ ] Save interesting results

### After Testing:
- [ ] Export test results if valuable
- [ ] Document lessons learned
- [ ] Note optimal parameter combinations
- [ ] Plan follow-up tests

---

## üéØ **CONCLUSION**

This comprehensive testing guide provides both structured core tests for demonstrations and extensive exploration opportunities for deep learning. The combination of four diverse technical documents (Empire State Building, Golden Gate Bridge, Hoover Dam, and International Space Station) creates rich testing scenarios across different engineering domains, time periods, and complexity levels.

**Key Testing Principles:**
- Start with core tests to understand basics
- Use extra testing to explore edge cases and optimizations
- Document findings for systematic improvement
- Focus on real-world applicability

**Remember:** The goal isn't just to test the system, but to understand how RAG works, when it excels, and how to optimize it for different use cases. Each test teaches you something valuable about retrieval-augmented generation!

---

*This guide will continue to evolve as we discover new testing scenarios and optimization opportunities. The systematic approach ensures consistent, reproducible results while building deep understanding of RAG system behavior.*

# üéØ **COMPREHENSIVE TEST QUESTION BANK**

*Based on the demo context files: donuts, pizza, ISS, Hoover Dam, Golden Gate Bridge, and Empire State Building*

---

## üéâ **FUN QUESTIONS**

### Food History Fun
- "What's the connection between Dutch settlers and modern donuts?"
- "Who invented the donut hole and why did they do it?"
- "Which came first: pizza or donuts, and by how many years?"
- "What role did World War II play in making donuts patriotic?"
- "Why do police officers love donuts so much?"
- "What's the most expensive donut ever sold and what made it so special?"
- "How did a sea captain from Maine change breakfast forever?"
- "What do ancient Egyptians and modern Americans have in common when it comes to fried food?"

### Engineering Marvels Fun
- "Which engineering project took longer: the Golden Gate Bridge or the Empire State Building?"
- "What do the Golden Gate Bridge and the ISS have in common in terms of color choices?"
- "How many Empire State Buildings could you stack to reach the ISS?"
- "Which weighs more: the Hoover Dam or 1000 Empire State Buildings?"

---

## üîó **CROSS-CONTEXT QUESTIONS**

### Time Period Connections
- "What major engineering projects were happening in America during the 1930s?"
- "Compare the construction timelines of the Empire State Building, Golden Gate Bridge, and Hoover Dam - what was happening in America during this era?"
- "How did the Great Depression affect the construction of major American landmarks?"

### International Collaboration vs American Independence
- "Compare the international cooperation required for the ISS versus the purely American efforts of 1930s construction projects"
- "How many countries worked together on the ISS compared to the Golden Gate Bridge project?"

### Materials and Innovation
- "What innovative materials or techniques were pioneered in 1930s construction that might have influenced modern space station design?"
- "Compare the concrete innovations of the Hoover Dam with the steel innovations of the Golden Gate Bridge"

### Scale and Perspective
- "If you laid all the wire from the Golden Gate Bridge end to end, how many times could you circle the ISS?"
- "How does the daily energy consumption of the ISS compare to the power generation of Hoover Dam?"
- "Which has more visitors per year: the Empire State Building or the ISS?"

### Cultural Impact Across Eras
- "How did donuts become as American as the Golden Gate Bridge?"
- "Compare the cultural significance of pizza in America versus the cultural significance of the Empire State Building"

---

## üß† **INTERESTING & THOUGHT-PROVOKING QUESTIONS**

### Historical What-Ifs
- "What would have happened to donut culture if Captain Gregory had never invented the hole?"
- "How might pizza have evolved differently if tomatoes had been accepted by wealthy Europeans earlier?"
- "What if the Golden Gate Bridge had been built as a cantilever bridge instead of suspension?"

### Technical Deep Dives
- "Explain the physics behind why the Golden Gate Bridge can sway 27 feet without breaking"
- "How does the ISS maintain its orbit without falling back to Earth?"
- "Why did the Hoover Dam engineers need to cool concrete for 2 years instead of 125?"
- "What makes the Empire State Building's foundation able to support 365,000 tons?"

### Innovation and Problem Solving
- "What was more innovative: putting a hole in a donut or putting cheese on flatbread?"
- "Compare the safety innovations of 1930s construction projects with modern ISS safety protocols"
- "How did the Salvation Army Doughnut Girls influence American food culture?"

### Modern Relevance
- "How do the environmental challenges of the ISS compare to the environmental impact of the Hoover Dam?"
- "What lessons from 1930s mega-projects could apply to modern space exploration?"
- "How has the gourmet donut revolution changed since the artisanal food movement began?"

---

## üé™ **SURPRISE ME QUESTIONS**

### Unexpected Connections
- "What do astronauts on the ISS and construction workers on the Golden Gate Bridge have in common regarding safety equipment?"
- "How is making the perfect pizza dough similar to mixing concrete for the Hoover Dam?"
- "What role did fog play in both Golden Gate Bridge construction and San Francisco's sourdough bread culture?"

### Numbers Game
- "How many donuts would you need to stack to reach the height of the Empire State Building?"
- "If you could eat one slice of pizza for every person who has visited the ISS, how many slices would that be?"
- "How many years would it take to eat all the donuts Americans consume annually if you ate one per day?"

### Alternative History
- "What if the ISS had been built in the 1930s using Hoover Dam construction techniques?"
- "How would pizza delivery work if everyone lived on the ISS?"
- "What if donuts had been invented in Italy and pizza in America?"

### Mind-Bending Comparisons
- "Which is more complex: the recipe for a perfect Neapolitan pizza or the life support systems of the ISS?"
- "What requires more precision: stretching pizza dough or positioning the ISS in orbit?"
- "Which took more international cooperation: creating modern pizza varieties or building the ISS?"

### Future Speculation
- "Will astronauts ever make fresh donuts in space, and what would be the challenges?"
- "How might pizza delivery change if we had regular flights to the ISS?"
- "What would a space-based version of the Golden Gate Bridge look like?"

### Philosophical Food for Thought
- "What does the evolution of donuts from ancient fried dough to Instagram-worthy art tell us about human creativity?"
- "How do mega-projects like the ISS compare to ancient wonders in terms of human achievement?"
- "What makes something become an icon: engineering excellence, cultural significance, or perfect timing?"

---

## üéØ **TESTING STRATEGY RECOMMENDATIONS**

### For Demonstrations
1. Start with **Fun Questions** to engage your audience
2. Use **Cross-Context Questions** to show multi-document capabilities
3. End with **Surprise Me Questions** to showcase the system's flexibility

### For Technical Validation
1. Use **Technical Deep Dives** to test complex retrieval
2. Try **Numbers Game** questions to test specific fact extraction
3. Use **Historical What-Ifs** to test reasoning capabilities

### For Parameter Optimization
1. Start with simple **Fun Questions** using default parameters
2. Progress to complex **Cross-Context Questions** that may require optimization
3. Use **Technical Deep Dives** to validate your optimized settings

### For Stress Testing
1. **Alternative History** questions test the system's boundaries
2. **Mind-Bending Comparisons** require sophisticated retrieval
3. **Future Speculation** tests how well the system handles hypothetical scenarios

---

*Remember: The best RAG testing combines systematic validation with creative exploration. These questions are designed to be both educational and entertaining while thoroughly exercising your RAG system's capabilities.*

---

# üîß **PARAMETER OPTIMIZATION MASTERCLASS**

*Understanding how, when, and why parameter changes affect your RAG system performance*

---

## ‚è∞ **CRITICAL TIMING: When Parameters Take Effect**

### üîÑ **DOCUMENT PROCESSING TIME PARAMETERS**
*These parameters only matter DURING document upload and chunking. Once processing is complete, changing these has NO EFFECT until you reprocess documents.*

#### **Chunk Size**
- **When It Matters**: During document upload and processing
- **Effect**: Determines how text is divided into searchable segments
- **To Apply Changes**: Must reprocess/re-upload documents
- **No Effect After**: Processing is complete - changing this won't affect existing chunks

#### **Chunk Overlap**
- **When It Matters**: During document upload and processing  
- **Effect**: Determines how much text is shared between adjacent chunks
- **To Apply Changes**: Must reprocess/re-upload documents
- **No Effect After**: Processing is complete - existing overlaps are fixed

### üîç **QUERY TIME PARAMETERS**
*These parameters take effect IMMEDIATELY when you ask questions. No reprocessing needed.*

#### **Number of Results (n_results)**
- **When It Matters**: During each question/query
- **Effect**: How many chunks are retrieved from the database
- **To Apply Changes**: Takes effect on next question immediately
- **Always Active**: Every query uses current setting

#### **Temperature, Max Tokens, Top P**
- **When It Matters**: During AI response generation
- **Effect**: Controls creativity, length, and randomness of answers
- **To Apply Changes**: Takes effect on next question immediately
- **Always Active**: Every response uses current settings

#### **System Prompt**
- **When It Matters**: During AI response generation
- **Effect**: Controls AI behavior and response style
- **To Apply Changes**: Takes effect on next question immediately
- **Always Active**: Every response uses current prompt

---

## üéØ **PARAMETER IMPACT ANALYSIS FOR YOUR QUESTION BANK**

### üìä **Number of Results (n_results) - QUERY TIME PARAMETER**

#### **Most Critical Parameter for Cross-Context Questions**

**‚ö° IMMEDIATE EFFECT - No Reprocessing Needed**

**Example Problem:**
- Question: *"Compare the construction timelines of the Empire State Building, Golden Gate Bridge, and Hoover Dam"*
- **n_results = 3**: Might only retrieve chunks about Empire State Building
- **n_results = 8-10**: Gets chunks from all three documents

**Why This Happens:**
- Each document has multiple chunks about construction
- With low n_results, similarity search might find 3 great Empire State chunks
- Higher n_results forces the system to look broader across documents

**Questions Most Affected:**
- ‚úÖ **All Cross-Context Questions** - Need multiple documents represented
- ‚úÖ **"Which weighs more: Hoover Dam or 1000 Empire State Buildings?"** - Needs weight data from both
- ‚úÖ **"What major engineering projects were happening in America during the 1930s?"** - Needs all three 1930s projects

**Recommended Settings:**
- **Simple single-document questions**: 3-5 results
- **Cross-context questions**: 8-12 results
- **Complex comparisons**: 10-15 results

---

### üìè **Chunk Size - PROCESSING TIME PARAMETER**

#### **Critical for Technical Deep Dives and Complex Information**

**üîÑ REQUIRES REPROCESSING - Must Re-upload Documents**

**Example Problem:**
- Question: *"Why did Hoover Dam engineers need to cool concrete for 2 years instead of 125?"*
- **Small chunks (300-500)**: Might split the cooling explanation across multiple chunks
- **Larger chunks (700-1000)**: Captures the complete technical explanation

**Why This Happens:**
- Technical explanations often span multiple paragraphs
- Small chunks break up cause-and-effect relationships
- Large chunks preserve context but may include irrelevant information

**Questions Most Affected:**
- ‚úÖ **"Explain the physics behind why the Golden Gate Bridge can sway 27 feet"** - Complex physics explanation
- ‚úÖ **"How does the ISS maintain its orbit without falling back to Earth?"** - Multi-step technical process
- ‚úÖ **"What makes the Empire State Building's foundation able to support 365,000 tons?"** - Engineering details

**Recommended Settings:**
- **Simple facts**: 400-600 characters
- **Technical explanations**: 700-1000 characters  
- **Historical narratives**: 800-1200 characters

---

### üîÑ **Chunk Overlap - PROCESSING TIME PARAMETER**

#### **Essential for Narrative Flow and Connected Information**

**üîÑ REQUIRES REPROCESSING - Must Re-upload Documents**

**Example Problem:**
- Question: *"How did the Salvation Army Doughnut Girls influence American food culture?"*
- **Low overlap (0-50)**: Might miss connections between wartime service and cultural impact
- **Higher overlap (100-150)**: Preserves the narrative flow from service to cultural significance

**Why This Happens:**
- Stories and explanations flow across chunk boundaries
- Key information often appears at transitions between topics
- Overlap ensures important connections aren't lost

**Questions Most Affected:**
- ‚úÖ **"What's the connection between Dutch settlers and modern donuts?"** - Historical progression
- ‚úÖ **"How did a sea captain from Maine change breakfast forever?"** - Cause and effect story
- ‚úÖ **"What role did World War II play in making donuts patriotic?"** - Historical narrative

**Recommended Settings:**
- **Factual queries**: 50-100 characters overlap
- **Historical narratives**: 100-150 characters overlap
- **Complex processes**: 125-200 characters overlap

---

### üéõÔ∏è **Temperature & Creativity Parameters - QUERY TIME PARAMETERS**

#### **Affects Reasoning and Synthesis Quality**

**‚ö° IMMEDIATE EFFECT - No Reprocessing Needed**

**Example Impact:**
- Question: *"What was more innovative: putting a hole in a donut or putting cheese on flatbread?"*
- **Low temperature (0.1-0.3)**: Factual comparison, less creative reasoning
- **Higher temperature (0.5-0.8)**: More creative analysis and synthesis

**Questions Most Affected:**
- ‚úÖ **All "Surprise Me" questions** - Require creative thinking
- ‚úÖ **"What makes something become an icon: engineering excellence, cultural significance, or perfect timing?"** - Philosophical reasoning
- ‚úÖ **Alternative history questions** - Need creative extrapolation

---

## üîß **SPECIFIC PARAMETER STRATEGIES FOR QUESTION TYPES**

### üéâ **Fun Questions Strategy**
```
PROCESSING TIME (Reprocess Required):
Chunk Size: 500-700
Overlap: 100

QUERY TIME (Immediate Effect):
Results: 5-8
Temperature: 0.4-0.6
```
**Why:** Most fun questions need specific facts but benefit from some creative presentation.

### üîó **Cross-Context Questions Strategy**
```
PROCESSING TIME (Reprocess Required):
Chunk Size: 600-800
Overlap: 125

QUERY TIME (Immediate Effect):
Results: 10-15
Temperature: 0.3-0.5
```
**Why:** Need broad retrieval across documents with factual accuracy for comparisons.

### üß† **Technical Deep Dives Strategy**
```
PROCESSING TIME (Reprocess Required):
Chunk Size: 800-1000
Overlap: 150

QUERY TIME (Immediate Effect):
Results: 6-10
Temperature: 0.2-0.4
```
**Why:** Technical explanations need large chunks to preserve complex information with high accuracy.

### üé™ **Surprise Me Questions Strategy**
```
PROCESSING TIME (Reprocess Required):
Chunk Size: 600-800
Overlap: 125

QUERY TIME (Immediate Effect):
Results: 8-12
Temperature: 0.6-0.8
```
**Why:** Creative questions need diverse information and higher creativity for synthesis.

---

## ‚ö†Ô∏è **COMMON FAILURE PATTERNS & SOLUTIONS**

### **Problem 1: "I don't have enough information"**
**Likely Cause:** Too few results retrieved
**Parameter Type:** üîç QUERY TIME - Immediate fix
**Solution:** Increase n_results from 3 ‚Üí 8-10
**Test With:** *"Compare international cooperation: ISS vs Golden Gate Bridge"*

### **Problem 2: Incomplete technical explanations**
**Likely Cause:** Chunk size too small, breaking up explanations
**Parameter Type:** üîÑ PROCESSING TIME - Requires reprocessing
**Solution:** Increase chunk size from 500 ‚Üí 800-1000, then re-upload documents
**Test With:** *"How does the ISS maintain its orbit without falling back to Earth?"*

### **Problem 3: Missing narrative connections**
**Likely Cause:** Insufficient chunk overlap
**Parameter Type:** üîÑ PROCESSING TIME - Requires reprocessing
**Solution:** Increase overlap from 50 ‚Üí 125-150, then re-upload documents
**Test With:** *"What's the connection between Dutch settlers and modern donuts?"*

### **Problem 4: Only getting info from one document in multi-doc questions**
**Likely Cause:** n_results too low + query expansion not finding diverse content
**Parameter Type:** üîç QUERY TIME - Immediate fix
**Solution:** Increase n_results AND ensure query expansion is enabled
**Test With:** *"What major engineering projects were happening in America during the 1930s?"*

### **Problem 5: Boring, factual answers to creative questions**
**Likely Cause:** Temperature too low
**Parameter Type:** üîç QUERY TIME - Immediate fix
**Solution:** Increase temperature from 0.2 ‚Üí 0.6-0.8
**Test With:** *"What if donuts had been invented in Italy and pizza in America?"*

---

## üß™ **DIAGNOSTIC TESTING APPROACH**

### **Step 1: Baseline Test**
Start with a challenging cross-context question:
*"How many countries worked together on the ISS compared to the Golden Gate Bridge project?"*

### **Step 2: Parameter Isolation**

#### **üîç QUERY TIME PARAMETERS (Test First - Immediate Results)**
1. **Results**: 3 ‚Üí 5 ‚Üí 8 ‚Üí 12 (find minimum for complete answer)
2. **Temperature**: 0.2 ‚Üí 0.4 ‚Üí 0.6 ‚Üí 0.8 (find optimal creativity level)

#### **üîÑ PROCESSING TIME PARAMETERS (Test If Query Time Fixes Don't Work)**
3. **Chunk Size**: 500 ‚Üí 700 ‚Üí 1000 (requires reprocessing each time)
4. **Overlap**: 50 ‚Üí 100 ‚Üí 150 (requires reprocessing each time)

### **Step 3: Optimization Validation**
Test your optimized settings on different question types:
- Simple fact: *"How tall is the Empire State Building?"*
- Cross-context: *"Which engineering project took longer: Golden Gate Bridge or Empire State Building?"*
- Technical: *"Explain the physics behind why the Golden Gate Bridge can sway 27 feet"*
- Creative: *"What do astronauts and Golden Gate Bridge workers have in common regarding safety?"*

---

## üéØ **PARAMETER TIMING CHEAT SHEET**

| Parameter | When It Takes Effect | Requires Reprocessing | Immediate Effect |
|-----------|---------------------|----------------------|------------------|
| **Chunk Size** | üîÑ Document Processing | ‚úÖ YES | ‚ùå NO |
| **Chunk Overlap** | üîÑ Document Processing | ‚úÖ YES | ‚ùå NO |
| **Number of Results** | üîç Each Query | ‚ùå NO | ‚úÖ YES |
| **Temperature** | üîç Each Query | ‚ùå NO | ‚úÖ YES |
| **Max Tokens** | üîç Each Query | ‚ùå NO | ‚úÖ YES |
| **Top P** | üîç Each Query | ‚ùå NO | ‚úÖ YES |
| **System Prompt** | üîç Each Query | ‚ùå NO | ‚úÖ YES |
| **Query Expansion** | üîç Each Query | ‚ùå NO | ‚úÖ YES |

---

## üéØ **OPTIMIZATION WORKFLOW**

### **Phase 1: Quick Wins (No Reprocessing)**
1. Test different **n_results** values (3, 5, 8, 12, 15)
2. Adjust **temperature** for question type (factual vs creative)
3. Enable/disable **query expansion** based on question complexity
4. Modify **system prompt** for specific use cases

### **Phase 2: Deep Optimization (Requires Reprocessing)**
*Only do this if Phase 1 doesn't solve your problems*
1. Analyze failed questions to identify if they need larger chunks
2. Test **chunk size** increases (500 ‚Üí 700 ‚Üí 1000)
3. Test **overlap** increases (50 ‚Üí 100 ‚Üí 150)
4. Reprocess documents and retest

### **Phase 3: Fine-Tuning**
1. Create parameter profiles for different question types
2. Document optimal settings for your specific use cases
3. Build testing protocols for new document types

---

## üìä **COMPLETE PARAMETER REFERENCE TABLE**

| Question Type | Chunk Size | Overlap | Results | Temperature | Reprocess? |
|---------------|------------|---------|---------|-------------|------------|
| Simple Facts | 400-600 | 75-100 | 3-5 | 0.2-0.4 | If chunks too small |
| Cross-Context | 600-800 | 100-125 | 8-12 | 0.3-0.5 | If missing connections |
| Technical Deep | 800-1000 | 125-150 | 6-10 | 0.2-0.4 | If explanations incomplete |
| Creative/Surprise | 600-800 | 100-125 | 8-12 | 0.6-0.8 | If missing context |
| Historical Narrative | 700-900 | 125-150 | 6-10 | 0.4-0.6 | If story fragmented |

**üîë Key Insight:** Always try adjusting QUERY TIME parameters first (Results, Temperature) before reprocessing documents with new PROCESSING TIME parameters (Chunk Size, Overlap).

--- 